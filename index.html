<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Robert Luo</title>
  
  <meta name="author" content="Robert Luo">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Robert Luo</name>
              </p>
              <p>I am now a second year master student from <a href="https://www.tsinghua.edu.cn/">Tsinghua University</a>, majoring in Artificial Intelligence and supervised by <a href="https://sites.google.com/view/iigroup-thu/about">Prof.Yang</a>. 
                My current research interests include Mathematical Expression Recognition and Multi-Modal Understanding&Generation.
              </p>
              <p>
                Previously, I obtained my bachelor degree from Southeast University and supervised by <a href="http://palm.seu.edu.cn/hxue/">Prof.Xue</a>.
              </p>
              <p style="text-align:center">
                <a href="robertluo171@gmail.com">Email</a> &nbsp/&nbsp

                <a href="data/Zhuoyan_Luo_Resume.pdf">CV</a> &nbsp/&nbsp
               
                <a href="https://github.com/RobertLuo1/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/zhuoyanluo.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/zhuoyanluo.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p><strong>[2024.12]</strong>The Repository <a href="https://github.com/TencentARC/SEED-Voken">SEED-Voken</a>: A Series of Powerful Visual Tokenizer, is released</p>
              <p><strong>[2024.09]</strong>The Technical Report Open-MAGVIT2 is released</p>
              <p><strong>[2024.05]</strong>The Bronze Prize of <a href="https://challenge.datacastle.cn/v3/cmptDetail.html?id=836">XIB OCR competition</a></p>
              <p><strong>[2024.02]</strong>The paper UVCOM is accepted by CVPR 2024</p>
              <p><strong>[2023.09]</strong>The paper SOC is accepted by NeurIPS 2023</p>
              <p><strong>[2023.09]</strong>The first prize of The 5th Large-scale Video Object Segmentation Challenge Track3: Referring Video Object Segmentation</p>
              <p><strong>[2023.07]</strong>The paper FATE is accepted by the NCAA</p>
              <p><strong>[2023.03]</strong>The second prize of ICDAR 2023 Multi-line Handwritten Mathematical Expression Recognition Competition</p>
              <p><strong>[2022.11]</strong>The second prize of <a href="http://challenge.xfyun.cn/topic/info?type=mathematical-formula">iFLYTEK A.I. HMER competition</a></p>
              <p><strong>[2022.09]</strong>Obtain the Principal Scholarship</p>
              <p><strong>[2022.06]</strong>A patent is accepted. Thanks for all help</p>
              <p><strong>[2021.11]</strong>Obtain National Scholarship</p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading style="padding:20px">Publications</heading>(&#42; equal contribution)
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/IBQ.png" alt="clean-usnob" width="180" height="100">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Taming Scalable Visual Tokenizer for Autoregressive Image Generation</papertitle>
              <br>
              <br>
              Fengyuan Shi*, <strong>Zhuoyan Luo*</strong>, Yixiao Ge, Yujiu Yang, Ying Shan, Limin Wang
              <br>
              <br>
              <em>Arxiv</em> / <a href="https://arxiv.org/pdf/2412.02692">paper</a> /<a href="https://github.com/TencentARC/SEED-Voken">code</a>
              <br>
              <!-- <br>
              <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2203.15332.pdf">arXiv</a> / <a href="https://github.com/GeWu-Lab/OGM-GE_CVPR2022">code
              </a>
              <br>
              <p>Alleviate optimization imbalance in multi-modal learning via on-the-fly gradient modulation.</p> -->
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Open-MAGVIT2.png" alt="clean-usnob" width="180" height="100">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Open-MAGVIT2: An Open-source Project Toward Democratizing Auto-Regressive Visual Generation</papertitle>
              <br>
              <br>
              <strong>Zhuoyan Luo*</strong>, Fengyuan Shi*, Yixiao Ge, Yujiu Yang, Limin Wang, Ying Shan 
              <br>
              <br>
              <em>Arxiv</em> / <a href="https://arxiv.org/pdf/2409.04410">paper</a> /<a href="https://github.com/TencentARC/SEED-Voken">code</a>
              <br>
              <!-- <br>
              <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2203.15332.pdf">arXiv</a> / <a href="https://github.com/GeWu-Lab/OGM-GE_CVPR2022">code
              </a>
              <br>
              <p>Alleviate optimization imbalance in multi-modal learning via on-the-fly gradient modulation.</p> -->
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/CoHD.png" alt="clean-usnob" width="180" height="100">
            </td>
            <td width="75%" valign="middle">
              <papertitle>CoHD: A Counting-Aware Hierarchical Decoding Framework for Generalized Referring Expression Segmentation</papertitle>
              <br>
              <br>
              <strong>Zhuoyan Luo*</strong>, Yinghao Wu*, Tianheng Cheng, Yong Liu, Yicheng Xiao, Hongfa Wang, Xiao-Ping Zhang, Yujiu Yang 
              <br>
              <br>
              <em>Arxiv</em> / <a href="https://arxiv.org/pdf/2405.15658">paper</a> /<a href="https://github.com/RobertLuo1/CoHD">code</a>
              <br>
              <!-- <br>
              <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2203.15332.pdf">arXiv</a> / <a href="https://github.com/GeWu-Lab/OGM-GE_CVPR2022">code
              </a>
              <br>
              <p>Alleviate optimization imbalance in multi-modal learning via on-the-fly gradient modulation.</p> -->
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/UVCOM.png" alt="clean-usnob" width="180" height="50">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Bridging the Gap: A Unified Video Comprehension Framework for Moment Retrieval and Highlight Detection</papertitle>
              <br>
              <br>
              Yicheng Xiao*, <strong>Zhuoyan Luo*</strong>, Yong Liu, Yue Ma, Hengwei Bian, Yatai Ji, Yujiu Yang, Xiu Li, 
              <br>
              <br>
              <em>CVPR 2024</em> / <a href="https://arxiv.org/pdf/2311.16464.pdf">paper</a> /<a href="https://github.com/EasonXiao-888/UVCOM">code</a>
              <br>
              <!-- <br>
              <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2203.15332.pdf">arXiv</a> / <a href="https://github.com/GeWu-Lab/OGM-GE_CVPR2022">code
              </a>
              <br>
              <p>Alleviate optimization imbalance in multi-modal learning via on-the-fly gradient modulation.</p> -->
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/SOC.png" alt="clean-usnob" width="180" height="80">
            </td>
            <td width="75%" valign="middle">
              <papertitle>SOC: Semantic-Assisted Object Cluster for Referring Video Object Segmentation</papertitle>
              <br>
              <br>
              <strong>Zhuoyan Luo*</strong>, Yicheng Xiao*, Yong Liu*, Shuyan Li, Yitong Wang, Yansong Tang, Xiu Li, Yujiu Yang
              <br>
              <br>
              <em>NeurIPS 2023</em> / <a href="https://arxiv.org/pdf/2305.17011.pdf">paper</a> /<a href="https://github.com/RobertLuo1/NeurIPS2023_SOC">code</a>
              <br>
              <!-- <br>
              <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2203.15332.pdf">arXiv</a> / <a href="https://github.com/GeWu-Lab/OGM-GE_CVPR2022">code
              </a>
              <br>
              <p>Alleviate optimization imbalance in multi-modal learning via on-the-fly gradient modulation.</p> -->
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/FATE.png" alt="clean-usnob" width="180" height="80">
            </td>
            <td width="75%" valign="middle">
              <papertitle>FATE: A Three-Stage Method for Arithmetical Exercise Correction</papertitle>
              <br>
              <br>
              Qipeng Zhu*,<strong>Zhuoyan Luo*</strong>, Shipeng Zhu*, Qi Jing, Zihang Xu, Hui Xue
              <br>
              <br>
              <em>Accepted by Journal of NCAA (Neural Computing and Applications)</em> / <a href="https://link.springer.com/article/10.1007/s00521-023-08890-6">paper</a>
              <br>
              <!-- <br>
              <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2203.15332.pdf">arXiv</a> / <a href="https://github.com/GeWu-Lab/OGM-GE_CVPR2022">code
              </a>
              <br>
              <p>Alleviate optimization imbalance in multi-modal learning via on-the-fly gradient modulation.</p> -->
            </td>
          </tr>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading style="padding:20px">Awards</heading>(&#42; equal contribution)
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/RVOS_ICCV_2023.png" alt="clean-usnob" width="180" height="100">
            </td>
            <td width="75%" valign="middle">
              <papertitle>The First Prize of ICCV 2023 <a href="https://youtube-vos.org/challenge/2023/leaderboard/">The 5th Large-scale Video Object Segmentation Challenge Track3: Referring Video Object Segmentation</a></papertitle>
              <br>
              <br>
              <strong>Zhuoyan Luo*</strong>, Yicheng Xiao*, Yong Liu*&Dagger;, Yitong Wang, Yansong Tang, Xiu Li, Yujiu Yang.
              <br>
              <br>
              *equal contribution, &Dagger;Project lead
              <!-- <br>
              <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2203.15332.pdf">arXiv</a> / <a href="https://github.com/GeWu-Lab/OGM-GE_CVPR2022">code
              </a>
              <br>
              <p>Alleviate optimization imbalance in multi-modal learning via on-the-fly gradient modulation.</p> -->
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ICDAR_2023.png" alt="clean-usnob" width="180" height="100">
            </td>
            <td width="75%" valign="middle">
              <papertitle>The Second Prize of ICDAR 2023 Multi-line Handwritten Mathematical Expression Recognition Competition</papertitle>
              <br>
              <br>
              <strong>Zhuoyan Luo*</strong>, Yinghao Wu*, Zihang Xu, Qi Jing, Hui Xue
              <br>
              <!-- <br>
              <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2203.15332.pdf">arXiv</a> / <a href="https://github.com/GeWu-Lab/OGM-GE_CVPR2022">code
              </a>
              <br>
              <p>Alleviate optimization imbalance in multi-modal learning via on-the-fly gradient modulation.</p> -->
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/HMER.png" alt="clean-usnob" width="180" height="80">
            </td>
            <td width="75%" valign="middle">
              <papertitle>The second Prize of <a href="http://challenge.xfyun.cn/topic/info?type=mathematical-formula">iFLYTEK A.I. HMER competition</a></papertitle>
              <br>
              <br>
              <strong>Zhuoyan Luo*</strong>, Yinghao Wu*
              <br>
              <!-- <br>
              <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2203.15332.pdf">arXiv</a> / <a href="https://github.com/GeWu-Lab/OGM-GE_CVPR2022">code
              </a>
              <br>
              <p>Alleviate optimization imbalance in multi-modal learning via on-the-fly gradient modulation.</p> -->
            </td>
          </tr>

          <!-- <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/balance2022.jpg" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Balanced Multimodal Learning via On-the-fly Gradient Modulation</papertitle>
              <br>
              <br>
              Xiaokang Peng*, <strong>Yake Wei*</strong>, <a href="https://antony0621.github.io/">Andong Deng</a>, Dong Wang, <a href="https://dtaoo.github.io">Di Hu</a>
              <br>
              <br>
              <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2203.15332.pdf">arXiv</a> / <a href="https://github.com/GeWu-Lab/OGM-GE_CVPR2022">code
              </a>
              <br>
              <p>Alleviate optimization imbalance in multi-modal learning via on-the-fly gradient modulation.</p>
            </td>
          </tr> -->

        </tbody></table>
				<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading style="padding:20px">Working Experience</heading>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/baai.jpeg" alt="clean-usnob" width="180" height="50">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Research Intern, BAAI Vision</papertitle>
              <br>
              <br>
              Jan.1,2025 - Present
              <br>
              <br>
              Supervised by: <a href="https://www.xloong.wang/">Xinlong Wang</a>
              <br>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/tencentarc.png" alt="clean-usnob" width="180" height="50">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Research Intern, Tencent ARC Laboratory</papertitle>
              <br>
              <br>
              Jan.1,2024 - Jan.1,2025
              <br>
              <br>
              Supervised by: <a href="https://geyixiao.com">Yixiao Ge</a>, Ying Shan
              <br>
            </td>
          </tr>
          <tr>
        <tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <p font-size:small;>
                  <br>
                  <br>
                  <div style="float:left;">
                      Updated at May. 2022
                  </div>
                  <div style="float:right;">
                      Thanks <a href="https://jonbarron.info">Jon Barron</a> for this amazing template.
                  </div>
                  <br>
                  <br>        
              </p>                           
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
